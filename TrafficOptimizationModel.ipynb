{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Optimization with AWS IoT and Machine Learning\n",
    "\n",
    "This notebook demonstrates steps to simulate IoT data for traffic optimization, process the data in real-time, train and deploy a predictive model using Amazon SageMaker, and analyze the data with AWS Athena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Setup\n",
    "In this section, we set up AWS configurations and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# AWS configuration\n",
    "role = get_execution_role()\n",
    "bucket = 'your-s3-bucket-name'\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulate IoT Data\n",
    "Here, we simulate IoT data for traffic lights and send it to AWS IoT Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to simulate IoT data and publish to AWS IoT Core\n",
    "from AWSIoTPythonSDK.MQTTLib import AWSIoTMQTTClient\n",
    "import json\n",
    "import time\n",
    "\n",
    "def generate_traffic_data():\n",
    "    return {\n",
    "        'traffic_light_id': 'TL-101',\n",
    "        'location': 'Main St & 1st Ave',\n",
    "        'vehicle_count': int(50 + 20 * time.time() % 1),\n",
    "        'average_speed': int(20 + 10 * time.time() % 1),\n",
    "        'CO2_level': round(0.04 + 0.02 * time.time() % 1, 4)\n",
    "    }\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing with Kinesis and Lambda\n",
    "Instructions to set up AWS Kinesis and Lambda for real-time data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration and Preparation\n",
    "Load processed data from S3 for exploration and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from S3\n",
    "s3 = boto3.client('s3')\n",
    "data_key = 'processed-data/traffic_data.csv'\n",
    "obj = s3.get_object(Bucket=bucket, Key=data_key)\n",
    "df = pd.read_csv(obj['Body'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training with SageMaker\n",
    "Train a model using the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and labels, and split data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "X = df[['vehicle_count', 'average_speed', 'CO2_level']]\n",
    "y = df['high_traffic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save model to file\n",
    "joblib.dump(model, 'model.joblib')\n",
    "\n",
    "# Upload model to S3\n",
    "s3.upload_file('model.joblib', bucket, 'model/model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy Model as a SageMaker Endpoint\n",
    "Deploy the trained model as a real-time endpoint using SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "model_path = f's3://{bucket}/model/model.joblib'\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_path,\n",
    "    role=role,\n",
    "    entry_point='inference.py',  # Assumes inference.py is in the same folder\n",
    "    framework_version='0.23-1'\n",
    ")\n",
    "\n",
    "# Deploy endpoint\n",
    "predictor = sklearn_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing the SageMaker Endpoint\n",
    "Send test data to the deployed endpoint to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test data\n",
    "test_data = {'features': [50, 20, 0.04]}\n",
    "prediction = predictor.predict(test_data)\n",
    "print('Prediction:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Analysis with Athena\n",
    "Set up Athena to query S3 data and analyze traffic trends."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
